{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to explain the creation of the embeddings dataset patentSBERTa - Not Executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this notebook, it will be explained the process to create the corresponding embeddings from the Content_file:\n",
    "\n",
    "- \"/bigstorage/DATASETS_JSON/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content_File\n",
    "\n",
    "Each patent within the Content_file is structured with the following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "- \"Application_Number\": \"2097973\",\n",
    "- \"Application_Date\": \"2013-04-03\",\n",
    "- \"Application_Category\": \"B1\",\n",
    "- \"Content\": {\n",
    "    - \"title\": \"DYNAMIC RANGE IMPROVEMENTS OF LOAD MODULATED AMPLIFIERS\",\n",
    "    - \"c-en-0001\": \"An antibody or antibody fragment according to claim 1, wherein said polypeptide is a recombinant polypeptide, and optionallywherein said recombinant polypeptide is biotinylated.\",\n",
    "    - \"c-en-0002\": ...,\n",
    "    - \"c-en-0003\": ...,\n",
    "    - \"c-en-0004\": ...,\n",
    "    - \"c-en-0005\": ...,\n",
    "    - \"c-en-0006\": ...,\n",
    "    - \"p0001\": \"The present invention relates generally to power amplifiers and amplifying methods and more specifically to high efficiency power amplifiers.\",\n",
    "    - \"p0002\": ...,\n",
    "    - \"p0003\": ...,\n",
    "    - \"p0004\": ...,\n",
    "    - \"p0005\": ...,\n",
    "    - \"p0006\": ...,\n",
    "    - \"p0007\": ...,\n",
    "    - \"p0008\": ...,\n",
    "    - \"p0010\": ...,\n",
    "    - \"p0011\": ...,\n",
    "    - \"p0013\": ...,\n",
    "    - \"p0014\": ...,\n",
    "    - ... ,\n",
    "    - \"p0099\": ...\n",
    "\n",
    "        }\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bigstorage/lufei/Projets/ChineseTerm/BertForDeprel/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/bigstorage/lufei/Projets/ChineseTerm/BertForDeprel/venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "# import faiss\n",
    "# from faiss import write_index, read_index\n",
    "from sentence_transformers import SentenceTransformer, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process to embed the content from each patent is as follows:\n",
    "\n",
    "1. We load and extract the information that we want to embed: Content information such as Abstract, Claims, Paragraphs and Figure references.\n",
    "\n",
    "2. For each element, the model SBERT tokenizes each word and then by applying the corresponding pooling strategy it determines the tokenized embedding for the whole text/sentence.\n",
    "\n",
    "    For instance, if we consider the first claim, it would tokenize each word, and by choosing mean pooling strategy it computes the average of all tokenized embeddings from the claim. \n",
    "    This new embedding is a vector of 768 dimensions.\n",
    "\n",
    "    BEFORE\n",
    "\n",
    "    - \"An antibody or antibody fragment according to claim 1, wherein said polypeptide is a recombinant polypeptide, and optionallywherein said recombinant polypeptide is biotinylated.\"\n",
    "\n",
    "    AFTER\n",
    "\n",
    "    - [-0.16003, -0.268536, -0.242169, -0.123718, 0.078339, -0.404047, 0.366489, -0.170657, -0.340436, 0.315698, ..., 0.031384, ,0.310652, 0.108265, 0.036854, -0.043038, -0.157491, 0.004529, 0.138287, -0.117745, -0.139216]\n",
    "\n",
    "3. Each embedded sentence is then stored in the file: \"/bigstorage/you/rag_project/embeddings_preindexed/embeddings_PatentSBERTa_mean.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling: This refers to the process of combining the output of the network over several dimensions into a fixed-size output. It's a common technique to reduce the dimensionality and to capture the essence of the input features.\n",
    "\n",
    "The common pooling strategies are:\n",
    "\n",
    "- Mean Pooling: Takes the average of all output vectors, effectively capturing the average effect of all the input tokens. For embeddings, mean pooling would average the embeddings of all tokens in the sequence, providing a single embedding vector representing the whole input.\n",
    "\n",
    "- Max Pooling: Takes the maximum value across the output vectors in each dimension, which can be thought of as capturing the most significant feature in each dimension of the embeddings. This is sometimes used to ensure that the strongest signal in each dimension is not diluted by averaging.\n",
    "\n",
    "- CLS Pooling: In models like BERT, the \"CLS\" token is a special token placed at the beginning of the input sequence. The embedding corresponding to this token can be used as the aggregate representation of the input sequence after the model has been fine-tuned, as it is designed to capture the context of the entire sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Rank documents using BM25')\n",
    "\n",
    "    # Chosen model is SBERT (Sentence-BERT) \n",
    "    parser.add_argument('--model', '-m', type=str, default='AI-Growth-Lab/PatentSBERTa', help='The model to use for embeddings')\n",
    "    \n",
    "    # Chosen pooling strategy\n",
    "    parser.add_argument('--pooling', '-p', type=str, default='mean', choices=['mean', 'max', 'cls'], help='The pooling strategy to use for embeddings')\n",
    "    \n",
    "    # Input file to embed\n",
    "    parser.add_argument('--input_file', '-i', type=str, help='The input file to create embeddings for', \n",
    "                        default='/bigstorage/DATASETS_JSON/Content_JSONs/Cited_2020_Uncited_2010-2019_Cleaned_Content_22k/CLEANED_CONTENT_DATASET_cited_patents_by_2020_uncited_2010-2019.json')\n",
    "    \n",
    "    # Output directory where the file will be saved\n",
    "    parser.add_argument('--output_dir', '-o', type=str, default='embeddings_preindexed', help='The output file to save the embeddings to')    \n",
    "    \n",
    "    # This line parses all of the arguments provided via the command line when the script is run.\n",
    "    # The parsed arguments become attributes of the args object, so you can access the values provided by the user with args.model, args.pooling, etc.\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "\n",
    "    # Load the input JSON file\n",
    "    with open(args.input_file, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    print(f\"Loaded {len(data)} documents from {args.input_file}\")\n",
    "    \n",
    "    # Convert the JSON to a pandas dataframe (long table of content)\n",
    "    columns = ['Application_Number', 'Application_Date', 'Application_Category', 'Content_Type', 'Content']\n",
    "\n",
    "    data_accumulator = []\n",
    "    for doc in data:\n",
    "        for content_type, content in doc['Content'].items():\n",
    "            # Create a dictionary for each row and append it to the list\n",
    "            row_data = {\n",
    "                'Application_Number': doc['Application_Number'], \n",
    "                'Application_Date': doc['Application_Date'], \n",
    "                'Application_Category': doc['Application_Category'], \n",
    "                'Content_Type': content_type, \n",
    "                'Content': content\n",
    "            }\n",
    "            data_accumulator.append(row_data)\n",
    "        print(row_data)\n",
    "        print(data_accumulator)\n",
    "        break\n",
    "\n",
    "    # Create the DataFrame from the accumulated data list only once\n",
    "    df = pd.DataFrame(data_accumulator, columns=columns)\n",
    "    print(df)\n",
    "\n",
    "    # Load the model\n",
    "    base_model = models.Transformer(args.model, max_seq_length=512)\n",
    "    pooling_model = models.Pooling(base_model.get_word_embedding_dimension(), pooling_mode=args.pooling)\n",
    "    model = SentenceTransformer(modules=[base_model, pooling_model]).to('cuda')\n",
    "\n",
    "    # Encode the corpus with potentially adjusted batch size\n",
    "    corpus_embeddings = model.encode(df['Content'].tolist(), show_progress_bar=True, batch_size=128)  # Adjust batch size as needed\n",
    "    print(f\"Encoded {len(corpus_embeddings)} documents\")\n",
    "\n",
    "    # Save the embeddings as a numpy array\n",
    "    if not os.path.exists(args.output_dir): # create the output directory if it doesn't exist\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    np.save(f'/bigstorage/Pablo_TER/test.npy', corpus_embeddings)\n",
    "    # np.save(f'{args.output_dir}/embeddings_{args.model.split(\"/\")[-1]}_{args.pooling}.npy', corpus_embeddings)\n",
    "    print(f\"Saved the embeddings to {args.output_dir}/embeddings_{args.model.split('/')[-1]}_{args.pooling}.npy\")\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading File + Shape + Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file \"embeddings_PatentSBERTa_mean.npy\":\n",
      " Shape: (2382315, 768)\n",
      " Size in GB: 6.815857887268066\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def open_npy_file(path_to_npy):\n",
    "    # Load the numpy array from the .npy file\n",
    "    npy_data = np.load(path_to_npy)\n",
    "    return npy_data\n",
    "\n",
    "path = \"/bigstorage/you/rag_project/embeddings_precalculated/embeddings_PatentSBERTa_mean.npy\"\n",
    "patentSBERTa = open_npy_file(path)\n",
    "\n",
    "\n",
    "size_in_bytes = patentSBERTa.nbytes\n",
    "# Convert the size to gigabytes\n",
    "size_in_gb = size_in_bytes / (1024 ** 3)\n",
    "\n",
    "print(\"The file \\\"embeddings_PatentSBERTa_mean.npy\\\":\")\n",
    "print(\" Shape:\", patentSBERTa.shape)\n",
    "print(\" Size in GB:\", size_in_gb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
