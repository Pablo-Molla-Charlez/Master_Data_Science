{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2963a1c",
   "metadata": {},
   "source": [
    "## Authors: Pablo Mollá, Pavlo Poliuha and Junjie Chen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73c13a-6a41-41a8-b3cb-bf0aaac86ebc",
   "metadata": {},
   "source": [
    "# Deep Learning - Laboratory Exercise 2\n",
    "\n",
    "- **WARNING:** you must have finished the previous exercise before this one as you will re-use parts of the code.\n",
    "\n",
    "- In the $\\textcolor{green}{\\text{first laboratory exercise}}$, we built a $\\textcolor{green}{\\text{simple linear classifier}}$. Although it can give reasonable results on the MNIST dataset (~92.5% of accuracy), deeper neural networks can achieve more the 99% accuracy. However, it can quickly become really impracical to explicitly code forward and backward passes. Hence, it is useful to rely on an auto-diff library where we specify the forward pass once, and the backward pass is automatically deduced from the computational graph structure.\n",
    "\n",
    "In $\\textcolor{red}{\\text{this laboratory exercise}}$, we will build a $\\textcolor{red}{\\text{small and simple auto-diff library}}$ that mimics the autograd mechanism from Pytorch (of course, we will simplify a lot!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3b7c425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(fname=\"mnist.pkl.gz\"):\n",
    "    f = gzip.open(fname, 'rb')\n",
    "\n",
    "    if(sys.version_info.major==2):\n",
    "        train_set, valid_set, test_set = pickle.load(f) # compatibility issue between python 2.7 and 3.4\n",
    "    else:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding='latin-1') # compatibility issue between python 2.7 and 3.4\n",
    "    f.close()\n",
    "\n",
    "    # Shuffle\n",
    "    train_X = train_set[0]\n",
    "    train_y = train_set[1]\n",
    "    valid_X = valid_set[0]\n",
    "    valid_y = valid_set[1]\n",
    "\n",
    "    train_perm = np.random.permutation(train_X.shape[0])\n",
    "    train_set = [train_X[train_perm,:],train_y[train_perm]]\n",
    "\n",
    "    valid_perm = np.random.permutation(valid_X.shape[0])\n",
    "    valid_set = [valid_X[valid_perm,:],valid_y[valid_perm]]\n",
    "\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "576d2e25-2822-46e1-a6de-80f793898317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import gzip\n",
    "import pickle\n",
    "import dataset_loader\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "#!pip install dataset_loader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b8ce741f-b2a7-422d-96de-d71cb79cfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    # this link doesn't work any more,\n",
    "    # seach on google for the file \"mnist.pkl.gz\"\n",
    "    # and download it\n",
    "    !wget http://deeplearning.net/data/mnist/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a4204abd-5cce-48f1-a78f-392a8275fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 3 splits\n",
    "train_data, dev_data, test_data = load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d35084-f93b-4245-b5cf-55baf1eb5f67",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "\n",
    "Instead of directly manipulating numpy arrays, we will manipulate abstraction that contains:\n",
    "- a value (i.e. a numpy array)\n",
    "- a bool indicating if we wish to compute the gradient with respect to the value\n",
    "- the gradient with respect to the value\n",
    "- the operation to call during backpropagation\n",
    "\n",
    "There will be two kind of nodes:\n",
    "- ComputationGraphNode: a generic computation node\n",
    "- Parameter: a computation node that is used to store parameters of the network. Parameters are always leaf nodes, i.e. they cannot be build from other computation nodes.\n",
    "\n",
    "Our implementation of the backward pass will be really simple and incorrect in the general case (i.e. won't work with computation graph with loops).\n",
    "We will just apply the derivative function for a given tensor and then call the ones of its antecedents, recursively.\n",
    "This simple algorithm is good enough for this exercise.\n",
    "\n",
    "Note that a real implementation of backprop will store temporary values during forward that can be used during backward to improve computation speed. We do not do that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a1535415-f6ce-486a-850a-120cd9008642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationGraphNode(object):\n",
    "    \n",
    "    def __init__(self, data, require_grad=False):\n",
    "        # we initialise the value of the node and the grad\n",
    "        if(not isinstance(data, np.ndarray)):\n",
    "            data = np.array(data)\n",
    "        self.value = data\n",
    "        self.grad = None\n",
    "        \n",
    "        self.require_grad = require_grad\n",
    "        self.func = None\n",
    "        self.input_nodes = None\n",
    "        self.func_parameters = []\n",
    "    \n",
    "    def set_input_nodes(self, *nodes):\n",
    "        self.input_nodes = list(nodes)\n",
    "\n",
    "    def set_func_parameters(self, *func_parameters):\n",
    "        self.func_parameters = list(func_parameters)\n",
    "    \n",
    "    def set_func(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def zero_grad(self):\n",
    "        if self.grad is not None:\n",
    "            self.grad.fill(0)\n",
    "\n",
    "    def set_gradient(self, gradient):\n",
    "        \"\"\"\n",
    "        Accumulate gradient for this tensor\n",
    "        \"\"\"\n",
    "        if gradient.shape != self.value.shape:\n",
    "            print(gradient.shape, self.value.shape)\n",
    "            raise RuntimeError(\"Invalid gradient dimension\")\n",
    "        if self.grad is None:\n",
    "            self.grad = gradient\n",
    "        else:\n",
    "            self.grad += gradient\n",
    "    \n",
    "    def backward(self, g=None):\n",
    "        if g is None:\n",
    "            g = self.value.copy()\n",
    "            g.fill(1.)\n",
    "        self.set_gradient(g)\n",
    "        if self.func is not None:\n",
    "            grad_list = self.func.backward(*(self.input_nodes + self.func_parameters + [g]))\n",
    "            for input_node, ngrad in zip(self.input_nodes, grad_list):\n",
    "                input_node.backward(ngrad)\n",
    "    \n",
    "    def __add__(self, y):\n",
    "        if not isinstance(y, ComputationGraphNode):\n",
    "            y = ComputationGraphNode(y)\n",
    "        return Addition()(self, y)\n",
    "\n",
    "    def __getitem__(self, slice):\n",
    "        return Selection()(self, slice)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.value.__str__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.value.__str__()\n",
    "\n",
    "class Parameter(ComputationGraphNode):\n",
    "    def __init__(self, data, name=\"default\"):\n",
    "        super().__init__(data, require_grad=True)\n",
    "        self.name  = name\n",
    "\n",
    "    def backward(self, g=None):\n",
    "        if g is not None:\n",
    "            self.set_gradient(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24dbd86-f592-4f8a-8657-272568a44f02",
   "metadata": {},
   "source": [
    "The class `Operation` is a class that three methods you should reimplement only the forward and the backward methods.\n",
    "* The `forward` method compute the function w.r.t inputs and return a new node that must contains information for backward pass.\n",
    "* The `backward` functions compute the gradient of the function w.r.t gradient of the output and other informations (forward pass input, parameter of the function...).**It should return a tuple**\n",
    "\n",
    "For better understanding below two operation are implemented, the selection and the addition (notice that it should not works yet since we do not defined what is a node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ca2caa2d-da91-4765-b709-b31acad0c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    @staticmethod\n",
    "    def forward(*args):\n",
    "        raise NotImplementedError(\"It is an abstract method\")\n",
    "    \n",
    "    def __call__(self, *args):\n",
    "        output_node = self.forward(*args)\n",
    "        output_node.set_func(self)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(*args):\n",
    "        pass\n",
    "class Addition(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        output_array = x.value + y.value\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x, y)\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        return (gradient, gradient)\n",
    "\n",
    "class Selection(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, slice):\n",
    "        np_x = x.value\n",
    "\n",
    "        output_array = np_x.__getitem__(slice)\n",
    "        \n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x)\n",
    "        output_node.set_func_parameters(slice)\n",
    "\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, slice, gradient):\n",
    "        np_x = x.value\n",
    "\n",
    "        cgrad = np_x.copy()\n",
    "        cgrad.fill(0)\n",
    "        cgrad.__setitem__(slice, gradient)\n",
    "        \n",
    "        return cgrad,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8a571-436a-499a-986a-f8a5ae89dab4",
   "metadata": {},
   "source": [
    "**Question 1** Complete the following class "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b7e641",
   "metadata": {},
   "source": [
    "### ReLU (Rectified Linear Unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7df06c",
   "metadata": {},
   "source": [
    "The ReLU (Rectified Linear Unit) function is a piecewise linear function that outputs the input directly if it is positive, and outputs zero otherwise. It is defined mathematically as:\n",
    "\n",
    "$$ ReLU(x) = \\max(0, x) $$\n",
    "\n",
    "#### Analytical Explanation of the Forward Pass\n",
    "\n",
    "The forward pass of the ReLU operation involves applying the ReLU function to each component of the input vector $x$. For each element $x_i$ of $x$, the ReLU function computes:\n",
    "\n",
    "$$ ReLU(x_i) = \n",
    "\\begin{cases} \n",
    "x_i & \\text{if } x_i > 0 \\\\\n",
    "0 & \\text{if } x_i \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- This operation is vectorized across the entire input vector $x$, resulting in an output vector where each element is the result of applying the ReLU function to the corresponding element of $x$. The purpose of applying ReLU is to introduce non-linearity into the model, allowing it to learn more complex patterns.\n",
    "\n",
    "#### Analytical Explanation of the Backward Pass\n",
    "\n",
    "During the backward pass, the gradient of the loss function with respect to the input of the ReLU operation needs to be calculated to propagate the error through the network. This gradient, denoted $\\frac{\\partial \\mathcal{L}}{\\partial x_i}$ for each input component $x_i$, depends on the derivative of the ReLU function.\n",
    "\n",
    "- The derivative of the ReLU function with respect to its input is:\n",
    "\n",
    "$$\n",
    "\\frac{d ReLU}{d x_i} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } x_i > 0 \\\\\n",
    "0 & \\text{if } x_i \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Thus, during the backward pass, for each component $x_i$ of $x$, the gradient of the loss $\\mathcal{L}$ with respect to $x_i$ is modified according to the derivative of ReLU. If $x_i$ was $\\textcolor{green}{\\text{positive}}$ (and thus passed through ReLU unchanged), the gradient $\\frac{\\partial \\mathcal{L}}{\\partial x_i}$ is passed backward unchanged. If $x_i$ was $\\textcolor{red}{\\text{non-positive}}$ (and ReLU output zero for it), the gradient with respect to $x_i$ is \"zeroed\" out because changes to $x_i$ would not affect the loss $\\mathcal{L}$ due to the flat (zero gradient) region of ReLU for $x_i \\leq 0$.\n",
    "\n",
    "- The $\\textcolor{orange}{\\text{overall operation can be represented as element-wise multiplication}}$ of the incoming gradient (from the subsequent layer or operation) by the gradient of ReLU with respect to its input. This gradient propagation respects the chain rule in calculus, which states that to compute the gradient of a composite function, you multiply the gradients of the composed functions.\n",
    "\n",
    "- In practical terms, this means for a given gradient vector $\\nabla \\mathcal{L}$ with respect to the output of ReLU, the backward pass computes:\n",
    "\n",
    "$$\n",
    "\\nabla \\mathcal{L} \\odot \\frac{d ReLU}{d x} = \n",
    "\\begin{cases} \n",
    "\\nabla \\mathcal{L}_i & \\text{if } x_i > 0 \\\\\n",
    "0 & \\text{if } x_i \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where $\\odot$ denotes element-wise multiplication, and $\\nabla \\mathcal{L}_i$ is the $i-th$ component of the gradient vector $\\nabla \\mathcal{L}$.\n",
    "\n",
    "This process ensures that the error signal is properly propagated backward through the network, allowing for the optimization of the model's weights during training via gradient descent or related algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a3f6684-1b56-40ad-8fe4-7dd9373f81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        # A copy of the input node's value is created to \n",
    "        # ensure the original data remains unchanged\n",
    "        np_x = x.value.copy()\n",
    "\n",
    "        # All negative elements in this copy are set to zero. \n",
    "        # This operation implements the ReLU function, which \n",
    "        # outputs the input directly if it's positive and zero \n",
    "        # otherwise.\n",
    "        np_x[np_x < 0] = 0\n",
    "\n",
    "        # A new ComputationGraphNode is instantiated with \n",
    "        # the processed values as its value. This new node \n",
    "        # represents the output of the ReLU operation.\n",
    "        output_node = ComputationGraphNode(np_x)\n",
    "\n",
    "        # The new node's input nodes are set to the input \n",
    "        # node x, establishing a link in the computational graph.\n",
    "        output_node.set_input_nodes(x)\n",
    "        \n",
    "        # Returns the output node, which is a ComputationGraphNode \n",
    "        # containing the result of applying the ReLU function to \n",
    "        # the input.\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, gradient):\n",
    "        # A copy of the input node's value is created to prevent \n",
    "        # modifying the original data.\n",
    "        np_x = x.value.copy()\n",
    "\n",
    "        # A mask is created where all elements of the input node's \n",
    "        # value that are less than zero are set to zero.\n",
    "        np_x[np_x < 0] = 0\n",
    "\n",
    "        # All elements of the mask that are greater than or \n",
    "        # equal to zero are set to one.\n",
    "        np_x[np_x >= 0] = 1\n",
    "\n",
    "        # Returns a tuple containing the product of the local \n",
    "        # gradients and the incoming gradient, which represents \n",
    "        # the gradient of the loss with respect to the input of \n",
    "        # the ReLU function.\n",
    "        return np_x * gradient,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee9ef9",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c543c5da",
   "metadata": {},
   "source": [
    "The following code defines a $\\textcolor{orange}{\\text{class TanH}}$ that represents the hyperbolic tangent activation function, a commonly used activation function in neural networks that scales the input to be between -1 and 1. The class provides methods for computing the forward pass ($\\textcolor{orange}{\\text{forward}}$) and the derivative for the backward pass ($\\textcolor{orange}{\\text{backward}}$) of the activation function within a computational graph framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c187a2d",
   "metadata": {},
   "source": [
    "#### 1. TanHCompute Method\n",
    "\n",
    "- **Purpose**: This static method computes the hyperbolic tangent of the input $x$. The computation is stabilized by subtracting the maximum absolute value of $x$ from $x$ to prevent numerical overflow in the exponentiation steps, especially for large positive or negative values of $x$.\n",
    "  \n",
    "- **Operation**: Given an input $x$, the method computes $\\tanh(x)$ using the formula:\n",
    "  \n",
    "  $$\n",
    "  \\tanh(x) = \\frac{e^{x - \\max(|x|)} - e^{-x - \\max(|x|)}}{e^{x - \\max(|x|)} + e^{-x - \\max(|x|)}}\n",
    "  $$\n",
    "  \n",
    "  This formula is derived from the definition of $\\tanh(x)$ but with a modification for numerical stability by incorporating $\\max(|x|)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27094505",
   "metadata": {},
   "source": [
    "#### 2. Forward Method\n",
    "\n",
    "- **Purpose**: Computes the forward pass of the TanH operation on a given input node $x$. This method applies the TanH activation function to each element of the input.\n",
    "\n",
    "- **Operation**: \n",
    "  - A copy of the input node's value $(x.value)$ is made to ensure the original data remains unchanged.\n",
    "  - The $TanHCompute$ method is called with this copy to apply the TanH activation function, resulting in an array where each input element is transformed by $\\tanh(x)$.\n",
    "  - A new $ComputationGraphNode$ is created with the transformed array as its value, representing the output of the TanH operation.\n",
    "  - The new node's input nodes are set to the input node $x$, linking the current operation in the computational graph.\n",
    "  \n",
    "- **Output**: The method returns the output node that contains the result of applying the TanH function to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416ba47",
   "metadata": {},
   "source": [
    "#### 3. Backward Method\n",
    "\n",
    "- **Purpose**: Computes the gradient of the loss function with respect to the input of the TanH operation during the backward pass of backpropagation.\n",
    "\n",
    "- **Operation**: \n",
    "  - A copy of the input node's value is again made.\n",
    "  \n",
    "  - The $TanHCompute$ method is applied to this copy to get the TanH activated values.\n",
    "  \n",
    "  - The derivative of the TanH function with respect to its input is computed using the identity $$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "  \n",
    "  This results in an array where each element is the local gradient of the TanH function at the corresponding input value.\n",
    "  \n",
    "  - The local gradients are then element-wise multiplied by the incoming gradient ($gradient$) from the subsequent operation or layer. This multiplication applies the chain rule, combining the local gradient with the gradient of the subsequent operations to get the gradient of the loss with respect to the TanH input.\n",
    "  \n",
    "- **Output**: The method returns a tuple containing the gradient of the loss with respect to the TanH input. This gradient is then used to update parameters earlier in the network during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "89adae37-5902-4e50-ac29-5ed332cc0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Operation):\n",
    "    @staticmethod\n",
    "    def TanHCompute(x):\n",
    "        max_x = np.abs(x).max()\n",
    "        return (np.exp(x-max_x) - np.exp(-x-max_x))/(np.exp(x-max_x) + np.exp(-x-max_x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(x):\n",
    "        np_x = x.value.copy()\n",
    "        np_x = TanH.TanHCompute(np_x)\n",
    "        output_node = ComputationGraphNode(np_x)\n",
    "        output_node.set_input_nodes(x)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, gradient):\n",
    "        np_x = x.value.copy()\n",
    "        np_x = TanH.TanHCompute(np_x)\n",
    "        np_x = 1 - np_x**2\n",
    "        return np_x * gradient,\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d873cf-a10f-4e33-ac84-0018933ab4c5",
   "metadata": {},
   "source": [
    "**Question 2:** Next, we implement the affine transform operation.\n",
    "You can reuse the code from the third lab exercise, with one major difference: you have to compute the gradient with respect to x too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b85c82",
   "metadata": {},
   "source": [
    "#### 1. Affine Transformation\n",
    "\n",
    "The $\\textcolor{orange}{\\text{class affine\\_transform}}$ models an affine transformation operation as part of a computational graph. An affine transformation is a linear mapping method that preserves points, straight lines, and planes. In neural networks, this operation is often referred to as a fully connected layer or a dense layer, represented mathematically as $y = Wx + b$, where:\n",
    "\n",
    "- W is the weight matrix\n",
    "\n",
    "- x is the input vector\n",
    "\n",
    "- b is the bias vector\n",
    "\n",
    "- y is the output vector\n",
    "\n",
    "#### 2. Forward Method\n",
    "\n",
    "- **Purpose**: Computes the $\\textcolor{orange}{\\text{forward pass of the affine transformation}}$.\n",
    "\n",
    "- **Operations**:\n",
    "\n",
    "    - $npx = x.value$: Extracts the actual value of the input node $x$.\n",
    "\n",
    "    - $npW = W.value$: Extracts the actual value of the weight matrix $W$.\n",
    "\n",
    "    - $npb = b.value$: Extracts the actual value of the bias vector $b$.\n",
    "\n",
    "    - $\\text{output\\_node = ComputationGraphNode(npW @ npx.T + npb)}$: Performs the affine transformation using matrix multiplication $@$ between $W$ and $x^T$ (transpose of $x$) and then adds the bias $b$. The result is wrapped in a $ComputationGraphNode$ to be used in further operations in the computational graph.\n",
    "\n",
    "    - $\\text{output\\_node.set\\_input\\_nodes(W, b, x)}$: Sets $W$, $b$, and $x$ as the input nodes to the current $\\text{output\\_node}$. This linkage is crucial for backpropagation, as it allows the graph to trace which nodes contributed to the final output.\n",
    "\n",
    "    - $\\text{return output\\_node}$: Returns the node containing the output of the affine transformation.\n",
    "\n",
    "### 3. Backward Method\n",
    "\n",
    "- **Purpose**: Computes the gradients of the loss function with respect to the affine transformation's parameters and inputs during the $\\textcolor{orange}{\\text{backward pass}}$.\n",
    "\n",
    "- **Operations**:\n",
    "\n",
    "    - $npx = x.value$: Retrieves the input vector $x$.\n",
    "\n",
    "    - $npW = W.value$: Retrieves the weight matrix $W$.\n",
    "    \n",
    "    - $npb = b.value$: Retrieves the bias vector $b$, though it's not directly used in the gradient calculations below.\n",
    "    \n",
    "    - $npg = gradient$: The gradient of the loss with respect to the output of the affine transformation.\n",
    "    \n",
    "    - $\\text{grad\\_x = np.dot(npW.T, npg)}$: Calculates the gradient with respect to the input $x$ by dotting the transpose of $W$ with the gradient $npg$. This follows from the chain rule in calculus and reflects how changes in $x$ influence the loss through the affine transformation.\n",
    "    \n",
    "    - $\\text{grad\\_w = np.matmul(npg.reshape(-1,1), npx.reshape(1,-1))}$: Computes the gradient with respect to the weights $W$. This is done by taking the outer product of the gradient $npg$ with respect to the output and the input $x$, which indicates how the loss changes with changes in $W$.\n",
    "\n",
    "    - $\\text{grad\\_b = npg}$: The gradient with respect to the bias $b$ is directly the gradient of the loss with respect to the output of the affine transformation, as each element of $b$ is added directly to each corresponding element of the output $y$, affecting the loss independently of the input $x$.\n",
    "    \n",
    "    - $\\text{return grad\\_w, grad\\_b, grad\\_x}$: Returns the gradients with respect to $W$, $b$, and $x$, which are used to update these parameters during the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "95cf9932-ddcf-472f-8732-c645b5063cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class affine_transform(Operation):\n",
    "    @staticmethod\n",
    "    def forward(W, b, x):\n",
    "        npx = x.value\n",
    "        npW = W.value\n",
    "        npb = b.value\n",
    "        output_node = ComputationGraphNode(npW @ npx.T + npb)\n",
    "        output_node.set_input_nodes(W, b, x)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(W, b, x, gradient):\n",
    "        npx = x.value\n",
    "        npW = W.value\n",
    "        npb = b.value\n",
    "        npg = gradient\n",
    "        grad_x = np.dot(npW.T, npg)\n",
    "        grad_w = np.matmul(npg.reshape(-1,1),npx.reshape(1,-1))\n",
    "        grad_b = npg\n",
    "        return grad_w, grad_b, grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f372bea6-9480-4c5c-b3c4-0bdc570abecb",
   "metadata": {},
   "source": [
    "**Question 3:** Define the NLL operation\n",
    "\n",
    "We recall that:\n",
    " \n",
    "- $$nll(x, y)= -log\\left(\\frac{e^{x_{y}}}{ \\sum\\limits_{i=1}^n e^{x_{ j}} }\\right) = -x_{y} + log(\\sum\\limits_{i=1}^n e^{x_{ j} })$$\n",
    "\n",
    "- $$\n",
    "    \\begin{align*}\n",
    "        \\frac{\\partial nll(x, y)}{\\partial x_i} &= - \\mathbb{1}_{y = i} + \\frac{\\partial log(\\sum\\limits_{i=1}^n e^{x_{ j} })}{\\partial\\sum\\limits_{i=1}^n e^{x_{ j} }}\\frac{\\sum\\limits_{i=1}^n e^{x_{ j} }}{\\partial x_i} \\\\\n",
    "        &= - \\mathbb{1}_{y = i} + \\frac{e^{x_i}}{\\sum\\limits_{i=1}^n e^{x_{ j} }} \n",
    "    \\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10e5e4",
   "metadata": {},
   "source": [
    "### Negative Log-Likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e0a5e",
   "metadata": {},
   "source": [
    "The $\\textcolor{orange}{\\text{class nll}}$ that models the $\\textcolor{green}{\\text{Negative log-likelihood (NLL)}}$ loss operation, commonly used in classification tasks. It utilizes the $\\textcolor{red}{\\text{softmax function}}$ to convert logits into probabilities, which are then used to compute the NLL loss. The class provides methods for computing both the forward and backward passes of this operation within a computational graph.\n",
    "\n",
    "#### 1. Softmax Method\n",
    "\n",
    "- **Purpose**: Computes the $\\textcolor{red}{\\text{softmax}}$ of a vector $x$. The softmax function is used to normalize an input vector into a probability distribution of predicted classes.\n",
    "\n",
    "- **Operations**:\n",
    "\n",
    "  - $\\text{e\\_x = np.exp(x - np.max(x))}$: Computes the exponential of each element in the input vector $x$, shifted by the maximum element in $x$ for numerical stability. Subtracting the max value prevents potential overflow by ensuring that the largest exponent argument is 0.\n",
    "\n",
    "  - $\\text{return e\\_x / e\\_x.sum()}$: Normalizes these exponentials to make their sum equal to 1, effectively converting the logits into probabilities.\n",
    "\n",
    "#### 2. Forward Method\n",
    "\n",
    "- **Purpose**: Computes the $\\textcolor{orange}{\\text{forward pass}}$ of the NLL loss for a predicted probability distribution and a true class index $y$.\n",
    "\n",
    "- **Operations**:\n",
    "  - $\\text{np\\_x = x.value.copy()}$: Creates a copy of the input vector $x$, which represents the logits before applying softmax.\n",
    "\n",
    "  - $\\text{np\\_x = nll.softmax(np\\_x)}$: Applies the $\\textcolor{red}{\\text{softmax}}$ method to the logits to obtain the predicted probabilities.\n",
    "\n",
    "  - $\\text{output\\_node = ComputationGraphNode(-np.log(np\\_x[y]))}$: Computes the $\\textcolor{green}{\\text{negative logarithm}}$ of the probability associated with the true class $y$. This quantity represents the NLL loss for the given prediction and true class. \n",
    "  \n",
    "  The result is wrapped in a $ComputationGraphNode$ to be used in further operations or for loss backpropagation.\n",
    "  \n",
    "  - $\\text{output\\_node.set\\_input\\_nodes(x)}$: Links the input logits node $x$ as an input to the current operation in the computational graph.\n",
    "\n",
    "  - $\\text{output\\_node.set\\_func\\_parameters(y)}$: Stores the true class index $y$ as a parameter of the operation for potential reference in the backward pass or other purposes.\n",
    "  \n",
    "#### 3. Backward Method\n",
    "\n",
    "- **Purpose**: Computes the gradient of the NLL loss with respect to the input logits $x$ during the $\\textcolor{orange}{\\text{backward pass}}$.\n",
    "\n",
    "- **Operations**:\n",
    "  - $\\text{np\\_x = x.value.copy()}$: Retrieves the input logits.\n",
    "  \n",
    "  - $\\text{g\\_x = nll.softmax(np\\_x)}$: Applies the $\\textcolor{red}{\\text{softmax}}$ to the logits to obtain the gradient of the softmax operation, which in this context represents the probabilities after applying softmax.\n",
    "\n",
    "  - $g\\_x[y] -= 1$: Adjusts the element of the softmax output corresponding to the true class $y$ by subtracting 1. This operation is based on the derivative of the $\\textcolor{green}{\\text{NLL}}$ loss with respect to the input logits, which simplifies to $p_i - 1$ for the true class $i=y$ and remains as $p_j$ for all other classes $j \\neq y$, where $p$ are the softmax probabilities.\n",
    "\n",
    "  - $\\text{return g\\_x * gradient,}$: Scales the computed gradient by the incoming gradient ($gradient$). In most cases, this incoming gradient will be 1, as the $\\textcolor{green}{\\text{NLL}}$ loss is typically the final operation whose gradient initiates the backward pass. The result is a vector of gradients with respect to each logit, which is used to update the weights that contributed to $x$ during optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3f742f0-83f3-4983-ae7a-cefd511b96d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nll(Operation):\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        np_x = x.value.copy()\n",
    "        np_x = nll.softmax(np_x)\n",
    "        output_node = ComputationGraphNode(-np.log(np_x[y]))\n",
    "        output_node.set_input_nodes(x)\n",
    "        output_node.set_func_parameters(y)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient=1):\n",
    "        np_x = x.value.copy()\n",
    "        g_x = nll.softmax(np_x)\n",
    "        g_x[y] -= 1\n",
    "        return g_x * gradient, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016217fb-a9f4-4908-b3e7-5fe372b38946",
   "metadata": {},
   "source": [
    "# Module\n",
    "\n",
    "Neural networks or parts of neural networks will be stored in Modules.\n",
    "They implement method to retrieve all parameters of the network and subnetwork."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43ea790d-a625-4aa4-95f1-8e4fe5e558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def __init__(self):\n",
    "        raise NotImplemented(\"\")\n",
    "        \n",
    "    def parameters(self):\n",
    "        ret = []\n",
    "        for name in dir(self):\n",
    "            o = self.__getattribute__(name)\n",
    "\n",
    "            if type(o) is Parameter:\n",
    "                ret.append(o)\n",
    "            if isinstance(o, Module) or isinstance(o, ModuleList):\n",
    "                ret.extend(o.parameters())\n",
    "        return ret\n",
    "\n",
    "# if you want to store a list of Parameters or Module,\n",
    "# you must store them in a ModuleList instead of a python list,\n",
    "# in order to collect the parameters correctly\n",
    "class ModuleList(list):\n",
    "    def parameters(self):\n",
    "        ret = []\n",
    "        for m in self:\n",
    "            if type(m) is Parameter:\n",
    "                ret.append(m)\n",
    "            elif isinstance(m, Module) or isinstance(m, ModuleList):\n",
    "                ret.extend(m.parameters())\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9688649-c900-4f00-bd06-38d93c1249cf",
   "metadata": {},
   "source": [
    "# Initialization and optimization\n",
    "\n",
    "**Question 1:** Implement the different initialisation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd147de",
   "metadata": {},
   "source": [
    "#### Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cbc73",
   "metadata": {},
   "source": [
    "- `Xavier initialization`, also known as Glorot initialization, is a method of initializing the weights of a neural network to help with the convergence during training. It is named after Xavier Glorot, who, along with `Yoshua Bengio`, introduced the method in a paper in 2010 to address the problems of training deep neural networks.\n",
    "\n",
    "- The `key` idea behind Xavier initialization is to initialize the weights in such a way that the variance of the outputs of each layer is the same as the variance of its inputs. This helps prevent the gradients from becoming too small or too large, a problem that can lead to either vanishing gradients or exploding gradients, respectively.\n",
    "\n",
    "- The Xavier initialization sets a layer's weights $W$ using a random distribution that has a zero mean and a specific variance that depends on the number of input and output units of the layer:\n",
    "\n",
    "$$ \\text{Var}(W) = \\frac{2}{n_{\\text{in}} + n_{\\text{out}}}$$\n",
    "\n",
    "Where:\n",
    "- $n_{\\text{in}}$ is the number of units (neurons) in the layer's input (i.e., the size of the previous layer).\n",
    "- $n_{\\text{out}}$ is the number of units in the layer's output (i.e., the size of the current layer).\n",
    "\n",
    "1. For the `uniform distribution`, the weights are initialized using the following range:\n",
    "\n",
    "$$ W \\sim \\text{Uniform}\\left(-\\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}, \\sqrt{\\frac{6}{n_{\\text{in}} + n_{\\text{out}}}}\\right) $$\n",
    "\n",
    "- We will use this one for our Neural Network.\n",
    "\n",
    "2. For the `normal distribution`, they are initialized using:\n",
    "\n",
    "$$ W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{\\text{in}} + n_{\\text{out}}}}\\right) $$\n",
    "\n",
    "In practice, when using libraries like TensorFlow or PyTorch, Xavier initialization can often be specified by using the appropriate initializer when constructing layers of a neural network. For example with TensorFlow, we could have used `tf.keras.initializers.GlorotUniform()` for uniform Xavier initialization or `tf.keras.initializers.GlorotNormal()` for normal Xavier initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbcbe52",
   "metadata": {},
   "source": [
    "#### Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457752dd",
   "metadata": {},
   "source": [
    "The Bias vector is set to 0 due to 2 main reasons:\n",
    "\n",
    "- `Kickstarting` Learning: We can think of the bias as a starting point for the neurons. Setting the bias to zero is like starting from a neutral position, allowing the weights (which are initialized using the Glorot method) to take the lead in learning from the data. This way, each neuron begins learning without any initial push in a particular direction, making it easier for the network to adjust and learn the patterns in the data effectively.\n",
    "\n",
    "- `Keeping Things Balanced`: By starting the biases at zero, we help keep the outputs of neurons initially centered around zero. This balance is crucial because it helps avoid extreme values in the neurons' outputs at the beginning of training, making it smoother for the network to learn and adjust as training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "337e899b-7551-42f5-ba48-bfef526f86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    b[:] = 0.\n",
    "\n",
    "def glorot_init(W):\n",
    "    W[:, :] = np.random.uniform(-np.sqrt(6. / (W.shape[0] + W.shape[1])), np.sqrt(6. / (W.shape[0] + W.shape[1])), W.shape)\n",
    "    \n",
    "# Look at slides for the formula!\n",
    "def kaiming_init(W):\n",
    "    W[:, :] = np.random.uniform(-np.sqrt(6. / W.shape[0]), np.sqrt(6. / W.shape[0]), W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ee98a-e90a-4ddb-b575-1688371efc05",
   "metadata": {},
   "source": [
    "**Question 2:** Implement the SGD \n",
    "\n",
    "We will implement the Stochastic gradient descent through an object, in the init function this object will store the different parameters (in a list format). The step function will update the parameters (see slides), notice that the gradient is stored in the nodes (grad attribute). Finally it will be necessary after each update to reset all the gradient to zero (in the method zero_grad) because we do not want to accumumlate gradient of all previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dbb7d09f-f4ae-472f-8b6a-f39fbaf8dccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient descent optimizer\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.value[:] = p.value - self.lr * p.grad\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76263efd-aed5-4a9a-b954-d9df83c9e8f4",
   "metadata": {},
   "source": [
    "# Networks and training loop\n",
    "\n",
    "We first create a simple linear classifier, similar to the first lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "50268482-0561-4e42-a85d-d7f592ca9769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNetwork(Module):\n",
    "    def __init__(self, dim_input, dim_output):\n",
    "        # build the parameters\n",
    "        self.W = Parameter(np.ndarray((dim_output, dim_input)), name=\"W\")\n",
    "        self.b = Parameter(np.ndarray((dim_output,)), name=\"b\")\n",
    "        self.init_parameters()\n",
    "        \n",
    "    def init_parameters(self):\n",
    "        # init parameters of the network (i.e W and b)\n",
    "        glorot_init(self.W.value)\n",
    "        zero_init(self.b.value)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return  affine_transform()(self.W, self.b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "49500117-74d1-4012-9096-53e3b8f298fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "55d9ac22-7355-4d0f-828c-7a14c34c42dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.18810699,  0.0176133 , -0.24328452, -0.37984876, -0.27928876,\n",
       "       -0.09780842, -0.07722615, -0.21863881,  0.23739514, -0.23216239])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those lines should be executed correctly\n",
    "lin1 = LinearNetwork(784, 10)\n",
    "lin2 = LinearNetwork(10, 5)\n",
    "x = ComputationGraphNode(train_data[0][0])\n",
    "a = lin1.forward(x + x)\n",
    "b = TanH()(a)\n",
    "c = lin2.forward(b)\n",
    "c.backward()\n",
    "x.grad[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b40f7e-5c56-4c49-bcc7-82b27730e3b6",
   "metadata": {},
   "source": [
    "We will train several neural networks.\n",
    "Therefore, we encapsulate the training loop in a function.\n",
    "\n",
    "**warning**: you have to call optimizer.zero_grad() before each backward pass to reinitialize the gradient of the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "367366bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(network, data):\n",
    "    correct = 0\n",
    "    total = len(data[0])\n",
    "    for i in range(total):\n",
    "        x = ComputationGraphNode(data[0][i])\n",
    "        y = data[1][i]\n",
    "        prediction = network.forward(x)\n",
    "        if np.argmax(prediction.value) == y:\n",
    "            correct += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "557d5a9a-c5b3-455b-9084-20470d1901db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(network, optimizer, train_data, dev_data, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        # shuffle the data\n",
    "        perm = np.random.permutation(train_data[0].shape[0])\n",
    "        total_loss = []\n",
    "        for i in perm:\n",
    "            # forward pass\n",
    "            x = ComputationGraphNode(train_data[0][i])\n",
    "            y = train_data[1][i]\n",
    "            a = network.forward(x)\n",
    "            loss = nll()(a, y)\n",
    "            total_loss.append(loss.value)\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        print(\"Epoch\", epoch, \"Loss\", np.mean(total_loss))\n",
    "        dev_accuracy = evaluate_accuracy(network, dev_data)\n",
    "        print(f\"Epoch {epoch}, Dev Accuracy: {dev_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d3e4305-1736-4987-a5d6-d0902dc78738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.37560995818388165\n",
      "Epoch 0, Dev Accuracy: 91.92%\n",
      "Epoch 1 Loss 0.310088821724386\n",
      "Epoch 1, Dev Accuracy: 92.08%\n",
      "Epoch 2 Loss 0.29789616490333165\n",
      "Epoch 2, Dev Accuracy: 91.65%\n",
      "Epoch 3 Loss 0.28972922938106144\n",
      "Epoch 3, Dev Accuracy: 92.46%\n",
      "Epoch 4 Loss 0.28505334983295916\n",
      "Epoch 4, Dev Accuracy: 92.58%\n",
      "Epoch 5 Loss 0.2816775362680396\n",
      "Epoch 5, Dev Accuracy: 92.24%\n",
      "Epoch 6 Loss 0.27935619283753604\n",
      "Epoch 6, Dev Accuracy: 92.39%\n",
      "Epoch 7 Loss 0.2760014512394773\n",
      "Epoch 7, Dev Accuracy: 92.59%\n",
      "Epoch 8 Loss 0.2738697245527043\n",
      "Epoch 8, Dev Accuracy: 92.60%\n",
      "Epoch 9 Loss 0.2728910719273141\n",
      "Epoch 9, Dev Accuracy: 92.57%\n",
      "Epoch 10 Loss 0.2709716254248619\n",
      "Epoch 10, Dev Accuracy: 92.58%\n",
      "Epoch 11 Loss 0.26963090054481\n",
      "Epoch 11, Dev Accuracy: 92.72%\n",
      "Epoch 12 Loss 0.26773596340631656\n",
      "Epoch 12, Dev Accuracy: 92.07%\n",
      "Epoch 13 Loss 0.2671389395590722\n",
      "Epoch 13, Dev Accuracy: 92.13%\n",
      "Epoch 14 Loss 0.2667036146651501\n",
      "Epoch 14, Dev Accuracy: 92.19%\n",
      "Epoch 15 Loss 0.26539462946901726\n",
      "Epoch 15, Dev Accuracy: 92.67%\n",
      "Epoch 16 Loss 0.26316978696927085\n",
      "Epoch 16, Dev Accuracy: 92.49%\n",
      "Epoch 17 Loss 0.26377853448029465\n",
      "Epoch 17, Dev Accuracy: 92.63%\n",
      "Epoch 18 Loss 0.2621002544260917\n",
      "Epoch 18, Dev Accuracy: 92.37%\n",
      "Epoch 19 Loss 0.2621290407099595\n",
      "Epoch 19, Dev Accuracy: 92.47%\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = LinearNetwork(dim_input, dim_output)\n",
    "optimizer = SGD(network.parameters(), 0.01)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab8c45-1492-4be0-a3bd-d257066b5876",
   "metadata": {},
   "source": [
    "After you finished the linear network, you can move to a deep network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180e14e-6490-4008-8fda-a1f7f7f71bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNetwork(ModuleList):\n",
    "    def __init__(self, dim_input, dim_output, hidden_dim, n_layers, tanh=False):\n",
    "        super().__init__()\n",
    "        layer_dims = [dim_input] + [hidden_dim] * n_layers + [dim_output] \n",
    "        # for exemple  [784] + [64, 64] + [10] = [784, 64, 64, 10]\n",
    "        print(layer_dims)\n",
    "        for i in range(n_layers+1):\n",
    "            W = Parameter(np.ndarray((layer_dims[i+1], layer_dims[i])), name=\"W\"+str(i))\n",
    "            b = Parameter(np.ndarray((layer_dims[i+1],)), name=\"b\"+str(i))\n",
    "            self.append(W)    \n",
    "            self.append(b)\n",
    "            if i < n_layers :\n",
    "                self.append(TanH() if tanh else ReLU())\n",
    "            # else :\n",
    "            #     self.activations.append(Softmax())\n",
    "        self.init_parameters()\n",
    "            \n",
    "        \n",
    "    def init_parameters(self):\n",
    "        # init parameters of the network (i.e W and b)\n",
    "        for param in self.parameters():\n",
    "            if isinstance(param, Parameter):\n",
    "                if \"W\" in param.name:\n",
    "                    glorot_init(param.value)\n",
    "                elif \"b\" in param.name:\n",
    "                    zero_init(param.value)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assume that every two parameters are followed \n",
    "        # by an activation function (except the last pair).\n",
    "        for i in range(0, len(self), 3):\n",
    "            W = self[i]\n",
    "            b = self[i+1]\n",
    "            x = affine_transform()(W, b, x)\n",
    "            if i + 2 < len(self):\n",
    "                activation = self[i+2]\n",
    "                x = activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2e256-b2fe-4e7f-8f5d-777ba54cc7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 128, 128, 10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.2831445339313861\n",
      "Epoch 0, Dev Accuracy: 95.49%\n",
      "Epoch 1 Loss 0.1349101524773894\n",
      "Epoch 1, Dev Accuracy: 96.65%\n",
      "Epoch 2 Loss 0.0930918653458311\n",
      "Epoch 2, Dev Accuracy: 97.05%\n",
      "Epoch 3 Loss 0.07056370702509744\n",
      "Epoch 3, Dev Accuracy: 97.15%\n",
      "Epoch 4 Loss 0.054038355620496456\n",
      "Epoch 4, Dev Accuracy: 97.42%\n",
      "Epoch 5 Loss 0.0411039193892383\n",
      "Epoch 5, Dev Accuracy: 97.23%\n",
      "Epoch 6 Loss 0.03226454712437156\n",
      "Epoch 6, Dev Accuracy: 97.43%\n",
      "Epoch 7 Loss 0.02553800082553301\n",
      "Epoch 7, Dev Accuracy: 97.69%\n",
      "Epoch 8 Loss 0.01801570397479008\n",
      "Epoch 8, Dev Accuracy: 97.73%\n",
      "Epoch 9 Loss 0.013206966522790872\n",
      "Epoch 9, Dev Accuracy: 97.68%\n",
      "Epoch 10 Loss 0.009633769959014113\n",
      "Epoch 10, Dev Accuracy: 97.74%\n",
      "Epoch 11 Loss 0.007157462010867529\n",
      "Epoch 11, Dev Accuracy: 97.77%\n",
      "Epoch 12 Loss 0.0049626592780931626\n",
      "Epoch 12, Dev Accuracy: 97.84%\n",
      "Epoch 13 Loss 0.003767523974072972\n",
      "Epoch 13, Dev Accuracy: 97.88%\n",
      "Epoch 14 Loss 0.002934241795351017\n",
      "Epoch 14, Dev Accuracy: 97.91%\n",
      "Epoch 15 Loss 0.002355867891097761\n",
      "Epoch 15, Dev Accuracy: 97.95%\n",
      "Epoch 16 Loss 0.001956810254175063\n",
      "Epoch 16, Dev Accuracy: 97.94%\n",
      "Epoch 17 Loss 0.001702323470502981\n",
      "Epoch 17, Dev Accuracy: 98.00%\n",
      "Epoch 18 Loss 0.001489986330862782\n",
      "Epoch 18, Dev Accuracy: 97.98%\n",
      "Epoch 19 Loss 0.0013249139645253325\n",
      "Epoch 19, Dev Accuracy: 97.98%\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = DeepNetwork(dim_input, dim_output, 128, 2,tanh=True)\n",
    "optimizer = SGD(network.parameters(), 0.005)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af7721-8e42-4b4f-aa58-fe3b02d3d867",
   "metadata": {},
   "source": [
    "## Better Optimizer\n",
    "Implement the SGD with momentum, notice that you will need to store the cumulated gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b804a3fc-9136-4d2f-8f18-a394821401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, params, lr=0.1, momentum=0.5):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.velocities = [np.zeros_like(p.value) for p in self.params]\n",
    "    def step(self):\n",
    "        for i, p in enumerate(self.params):\n",
    "            self.velocities[i] = self.momentum * self.velocities[i] - self.lr * p.grad\n",
    "            p.value[:] = p.value + self.velocities[i]\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c2456f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 128, 128, 10]\n",
      "Epoch 0 Loss 0.25469738291169614\n",
      "Epoch 0, Dev Accuracy: 95.60%\n",
      "Epoch 1 Loss 0.11799272566302463\n",
      "Epoch 1, Dev Accuracy: 96.96%\n",
      "Epoch 2 Loss 0.08095845824358455\n",
      "Epoch 2, Dev Accuracy: 97.16%\n",
      "Epoch 3 Loss 0.060305005829908685\n",
      "Epoch 3, Dev Accuracy: 96.74%\n",
      "Epoch 4 Loss 0.04269164448416475\n",
      "Epoch 4, Dev Accuracy: 96.76%\n",
      "Epoch 5 Loss 0.03309211523605838\n",
      "Epoch 5, Dev Accuracy: 97.52%\n",
      "Epoch 6 Loss 0.02164025983200781\n",
      "Epoch 6, Dev Accuracy: 97.34%\n",
      "Epoch 7 Loss 0.015530513718558871\n",
      "Epoch 7, Dev Accuracy: 97.51%\n",
      "Epoch 8 Loss 0.010896108839154638\n",
      "Epoch 8, Dev Accuracy: 97.68%\n",
      "Epoch 9 Loss 0.005789169458003369\n",
      "Epoch 9, Dev Accuracy: 97.81%\n",
      "Epoch 10 Loss 0.002748290812503277\n",
      "Epoch 10, Dev Accuracy: 98.00%\n",
      "Epoch 11 Loss 0.0014415828710679101\n",
      "Epoch 11, Dev Accuracy: 97.95%\n",
      "Epoch 12 Loss 0.0009706740619321562\n",
      "Epoch 12, Dev Accuracy: 97.95%\n",
      "Epoch 13 Loss 0.00079955032895923\n",
      "Epoch 13, Dev Accuracy: 98.00%\n",
      "Epoch 14 Loss 0.0006942616789336818\n",
      "Epoch 14, Dev Accuracy: 97.99%\n",
      "Epoch 15 Loss 0.0006134691853626342\n",
      "Epoch 15, Dev Accuracy: 97.99%\n",
      "Epoch 16 Loss 0.0005533541473690156\n",
      "Epoch 16, Dev Accuracy: 97.96%\n",
      "Epoch 17 Loss 0.0005032078707100316\n",
      "Epoch 17, Dev Accuracy: 97.98%\n",
      "Epoch 18 Loss 0.00045983523200132756\n",
      "Epoch 18, Dev Accuracy: 97.96%\n",
      "Epoch 19 Loss 0.0004261448810565424\n",
      "Epoch 19, Dev Accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "dim_input = 28*28\n",
    "dim_output = 10\n",
    "\n",
    "network = DeepNetwork(dim_input, dim_output, 128, 2,tanh=True)\n",
    "optimizer = SGD(network.parameters(), 0.005)\n",
    "\n",
    "training_loop(network, optimizer, train_data, dev_data, n_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8739e57-a82b-4908-9488-1b83d6e9a51f",
   "metadata": {},
   "source": [
    "## Bonus: Batch SGD\n",
    "Propose a method to take into account batch of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824876ff-1dca-44ec-a777-620dfdd0d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSGD:\n",
    "    def __init__(self, params, lr=0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self, batch_size):\n",
    "        for p in self.params:\n",
    "            p.value[:] = p.value - self.lr * p.grad / batch_size\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_loop_batch(network, optimizer, train_data, dev_data, batch_size=32, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle the data\n",
    "        perm = np.random.permutation(train_data[0].shape[0])\n",
    "        train_data_shuffled = train_data[0][perm], train_data[1][perm]\n",
    "        total_loss = []\n",
    "\n",
    "        for i in range(0, train_data_shuffled[0].shape[0], batch_size):\n",
    "            # Extracting a mini-batch\n",
    "            x_batch = train_data_shuffled[0][i:i+batch_size]\n",
    "            y_batch = train_data_shuffled[1][i:i+batch_size]\n",
    "\n",
    "            # Forward pass for a mini-batch\n",
    "            x = ComputationGraphNode(x_batch)\n",
    "            y_pred = network.forward(x)\n",
    "\n",
    "            losses = [nll()(y_pred, y) for y in y_batch]\n",
    "            batch_loss = sum(loss.value for loss in losses) / len(losses)\n",
    "            total_loss.append(batch_loss)\n",
    "\n",
    "            # Backward pass\n",
    "            for loss in losses:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Compute and print the average loss\n",
    "        avg_loss = np.mean(total_loss)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        dev_accuracy = evaluate_accuracy(network, dev_data)\n",
    "        print(f\"Epoch {epoch+1}, Dev Accuracy: {dev_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    @staticmethod\n",
    "    def forward(*args):\n",
    "        raise NotImplementedError(\"It is an abstract method\")\n",
    "\n",
    "    @classmethod\n",
    "    def __call__(self, *args):\n",
    "        output_node = self.forward(*args)\n",
    "        output_node.set_func(self)\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(*args):\n",
    "        pass\n",
    "\n",
    "class Addition(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        output_array = x.vg_tensor.data + y.vg_tensor.data\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "\n",
    "        output_node.set_input_nodes(x, y)\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        return (gradient, gradient)\n",
    "\n",
    "class Power(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, power):\n",
    "        np_x = x.vg_tensor.data\n",
    "        np_power = power.vg_tensor.data\n",
    "        output_array = np_x**np_power\n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "\n",
    "        output_node.set_input_nodes(x, power)\n",
    "        return output_node\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(x, power, gradient):\n",
    "        np_x = x.vg_tensor.data\n",
    "        np_power = power.vg_tensor.data\n",
    "        \n",
    "        return  (np_power * np_x**(np_power-1) * gradient, np.log(np_x) * np_x**(np_power) * gradient)\n",
    "\n",
    "class Multiplication(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        np_x = x.vg_tensor.data\n",
    "        np_y = y.vg_tensor.data\n",
    "\n",
    "        output_array = np_x * np_y \n",
    "        \n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x, y)\n",
    "\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        np_x = x.vg_tensor.data\n",
    "        np_y = y.vg_tensor.data\n",
    "        \n",
    "        return (np_y * gradient, np_x * gradient) \n",
    "        \n",
    "class Division(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, y):\n",
    "        np_x = x.vg_tensor.data\n",
    "        np_y = y.vg_tensor.data\n",
    "\n",
    "        output_array = np_x / np_y \n",
    "        \n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x, y)\n",
    "\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, y, gradient):\n",
    "        np_x = x.vg_tensor.data\n",
    "        np_y = y.vg_tensor.data\n",
    "        \n",
    "        return (gradient/np_y, -(np_x/(np_y**2) * gradient)) \n",
    "\n",
    "class Selection(Operation):\n",
    "    @staticmethod\n",
    "    def forward(x, slice):\n",
    "        np_x = x.vg_tensor.data\n",
    "\n",
    "        output_array = np_x.__getitem__(slice)\n",
    "        \n",
    "        output_node = ComputationGraphNode(output_array)\n",
    "        output_node.set_input_nodes(x)\n",
    "        output_node.set_func_parameters(slice)\n",
    "\n",
    "        return output_node\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(x, slice, gradient):\n",
    "        np_x = x.vg_tensor.data\n",
    "\n",
    "        cgrad = np_x.copy()\n",
    "        cgrad.fill(0)\n",
    "        cgrad.__setitem__(slice, gradient)\n",
    "        \n",
    "        return cgrad\n",
    "        \n",
    "class VGTensor(object):\n",
    "    def __init__(self, array_like):\n",
    "        self.data = np.array(array_like, dtype=np.float64)\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float64)\n",
    "        \n",
    "class ComputationGraphNode(object):\n",
    "    \n",
    "    def __init__(self, data, require_grad=False):\n",
    "        # We initialise the value of the node and the grad\n",
    "        self.value = np.array(data)\n",
    "        self.vg_tensor = VGTensor(data)\n",
    "        if require_grad:\n",
    "            self.grad = np.zeros_like(self.value)\n",
    "        else:\n",
    "            self.grad = None\n",
    "        self.require_grad = require_grad\n",
    "        self.func = None\n",
    "        self.input_nodes = None\n",
    "        self.func_parameters = []\n",
    "    \n",
    "    def set_input_nodes(self, *nodes):\n",
    "        self.input_nodes = list(nodes)\n",
    "\n",
    "    def set_func_parameters(self, *func_parameters):\n",
    "        self.func_parameters = list(func_parameters)\n",
    "    \n",
    "    def set_func(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.vg_tensor.grad.fill(0)\n",
    "\n",
    "    def set_gradient(self, gradient):\n",
    "        if self.vg_tensor.grad is None:\n",
    "            self.vg_tensor.grad = gradient\n",
    "        else:\n",
    "            try:\n",
    "                if gradient.shape != self.vg_tensor.grad.shape:\n",
    "                    if gradient.shape == ():\n",
    "                        gradient = np.full(self.vg_tensor.grad.shape, gradient)\n",
    "                    else:\n",
    "                        gradient = np.reshape(gradient, self.vg_tensor.grad.shape)\n",
    "                self.vg_tensor.grad += gradient\n",
    "            except ValueError as e:\n",
    "                raise ValueError(f\"Cannot reshape gradient of shape {gradient.shape} to {self.vg_tensor.grad.shape}\") from e\n",
    "\n",
    "\n",
    "    \n",
    "    def backward(self, g=None):\n",
    "        if g is None:\n",
    "            g = self.vg_tensor.data.copy()\n",
    "            g.fill(1.)\n",
    "        else:\n",
    "            self.set_gradient(g)\n",
    "        if self.func is not None:\n",
    "            grad_list = self.func.backward(*(self.input_nodes+ self.func_parameters + [g]))\n",
    "            for input_node, ngrad in zip(self.input_nodes, grad_list):\n",
    "                input_node.backward(ngrad)\n",
    "    \n",
    "    def show_graph(self):\n",
    "        if self.func is not None:\n",
    "            for x in self.input_nodes:\n",
    "                x.show_graph()\n",
    "    \n",
    "    def __add__(self, y):\n",
    "        if not isinstance(y, ComputationGraphNode):\n",
    "            y = ComputationGraphNode(y)\n",
    "        return Addition()(self, y)\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        if not isinstance(power, ComputationGraphNode):\n",
    "            power = ComputationGraphNode(power)\n",
    "        return Power()(self, power)\n",
    "\n",
    "    def __mul__(self, y):\n",
    "        if not isinstance(y, ComputationGraphNode):\n",
    "            y = ComputationGraphNode(y)\n",
    "        return Multiplication()(self, y)\n",
    "        \n",
    "    def __truediv__(self, y):\n",
    "        if not isinstance(y, ComputationGraphNode):\n",
    "            y = ComputationGraphNode(y)\n",
    "        return Division()(self, y)\n",
    "\n",
    "    def __getitem__(self, slice):\n",
    "        return Selection()(self, slice)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.vg_tensor.data.__str__()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.vg_tensor.data.__str__()\n",
    "\n",
    "class Parameter(ComputationGraphNode):\n",
    "    def __init__(self, data, name=\"default\"):\n",
    "        super().__init__(data, require_grad=True)\n",
    "        self.name  = name\n",
    "\n",
    "    def backward(self, g):\n",
    "        if g is not None:\n",
    "            self.set_gradient(g)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b14215",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = Parameter([0.1, 0.1])\n",
    "b = Parameter([0.1])\n",
    "x = ComputationGraphNode([0.5,-0.5])\n",
    "y = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17218488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.10517092]\n",
      "[1.15458608]\n",
      "[1.20829638]\n",
      "[1.26685385]\n",
      "[1.33090403]\n",
      "[1.40120558]\n",
      "[1.47865446]\n",
      "[1.56431441]\n",
      "[1.65945488]\n",
      "[1.76559901]\n",
      "[1.88458382]\n",
      "[2.01863608]\n",
      "[2.17046732]\n",
      "[2.34339175]\n",
      "[2.54146998]\n",
      "[2.76967794]\n",
      "[3.03409263]\n",
      "[3.34206746]\n",
      "[3.70233212]\n",
      "[4.12487816]\n",
      "[4.62036009]\n",
      "[5.19853876]\n",
      "[5.86506792]\n",
      "[6.61593388]\n",
      "[7.42980587]\n",
      "[8.26147487]\n",
      "[9.04398733]\n",
      "[9.70713099]\n",
      "[10.2068694]\n",
      "[10.54279287]\n",
      "[10.74804037]\n",
      "[10.86509911]\n",
      "[10.92898661]\n",
      "[10.96296747]\n",
      "[10.9807852]\n",
      "[10.99005661]\n",
      "[10.99486156]\n",
      "[10.99734653]\n",
      "[10.99863026]\n",
      "[10.99929307]\n",
      "[10.99963519]\n",
      "[10.99981175]\n",
      "[10.99990286]\n",
      "[10.99994987]\n",
      "[10.99997413]\n",
      "[10.99998665]\n",
      "[10.99999311]\n",
      "[10.99999645]\n",
      "[10.99999817]\n",
      "[10.99999905]\n",
      "[10.99999951]\n",
      "[10.99999975]\n",
      "[10.99999987]\n",
      "[10.99999993]\n",
      "[10.99999997]\n",
      "[10.99999998]\n",
      "[10.99999999]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n",
      "[11.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_699811/747365483.py:45: RuntimeWarning: invalid value encountered in log\n",
      "  return  (np_power * np_x**(np_power-1) * gradient, np.log(np_x) * np_x**(np_power) * gradient)\n",
      "/tmp/ipykernel_699811/747365483.py:106: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  cgrad.__setitem__(slice, gradient)\n",
      "/tmp/ipykernel_699811/747365483.py:45: RuntimeWarning: divide by zero encountered in log\n",
      "  return  (np_power * np_x**(np_power-1) * gradient, np.log(np_x) * np_x**(np_power) * gradient)\n",
      "/tmp/ipykernel_699811/747365483.py:45: RuntimeWarning: invalid value encountered in multiply\n",
      "  return  (np_power * np_x**(np_power-1) * gradient, np.log(np_x) * np_x**(np_power) * gradient)\n"
     ]
    }
   ],
   "source": [
    "epsilon = 2e-3\n",
    "\n",
    "for i in range(100):\n",
    "    o = w1[0] * x[0] + w1[1] * x[1] + b\n",
    "    loss = (ComputationGraphNode([math.e])**o + (- y))**2 \n",
    "    loss.backward()\n",
    "    print(ComputationGraphNode([math.e])**o )\n",
    "    w1.vg_tensor.data = w1.vg_tensor.data - epsilon * w1.vg_tensor.grad\n",
    "    b.vg_tensor.data = b.vg_tensor.data - epsilon * b.vg_tensor.grad\n",
    "    w1.zero_grad()\n",
    "    b.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa10b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, *args, **kwargs):\n",
    "        pass\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        last_node = self.forward(*args, **kwargs)\n",
    "        return last_node\n",
    "        \n",
    "    def parameters(self):\n",
    "        parameters = []\n",
    "        for name in dir(self):\n",
    "            attribute = self.__getattribute__(name)\n",
    "            if isinstance(attribute, 'Parameter'):\n",
    "                parameters.append(attribute)\n",
    "            elif isinstance(attribute, 'Module'):\n",
    "                parameters += attribute.parameters()\n",
    "        return parameters\n",
    "            \n",
    "class Linear(Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__() \n",
    "        self.W = Parameter(np.array((in_dim, out_dim)), name='W')\n",
    "        self.b = Parameter(np.array((out_dim)), name='b')\n",
    "    def forward(self, x):\n",
    "        return np.dot(self.W.vg_tensor.data, x) + self.b.vg_tensor.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
